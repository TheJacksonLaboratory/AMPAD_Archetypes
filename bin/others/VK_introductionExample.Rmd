---
title: "Manual for Pareto front and task inference, ParetoTI R package"
author: "Vitalii Kleshchevnikov"
date: "10/10/2018"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction and relevant papers

  This package allows to find tasks that cells need to perform and trade-offs between them. 
  
  Need to perform multiple objectives and natural selection put cells on a Pareto front, a narrow subspace of optimal performance. When cells move along this front they trade-off performance at those tasks. Pareto front in the performance space translates into simple shapes gene expression of cell population. By finding minimal simplex polytope (triangle in 2D, tetrahedron in 3D, 5-vertex polytope in 4D) that encloses most of the data you can indentify the cellular tasks. This relies on recent work by Uri Alon group that showed that Pareto front is equal to minimal polytope defined by specialist phenotypes (convex hull defined by archetypes) and developed a matlab package ParTI for performing this analysis.
  
  We would strongly reccomend to familiarise yourself with the theory and prevous applications by reading 2015 Nature Methods paper and reviewing lecture materials from Uri Alon's "Physics of behavour" course at Weizmann Institute:    
  1. [Inferring biological tasks using Pareto analysis of high-dimensional data, 2015, Nature Methods](https://www.nature.com/articles/nmeth.3254)    
  2. [Systems biology course 2018 Uri Alon - Lecture 10 Optimality in Biological Circuits.](https://www.youtube.com/watch?v=PfPWpWjvRPU)    
  3. [Universal cancer tasks, evolutionary tradeoffs, and the functions of driver mutations](https://www.biorxiv.org/content/early/2018/08/01/382291)    
  

## 2. Installing ParetoTI R package

You need to install development version of this package from github.com and install a python dependency (PCHA algorithm implementation). If you want to take advantage of high-performance computing cluster you need to do extra set up, please follow the instructions [here](https://github.com/mschubert/clustermq).    

```{r install, eval=FALSE}
# Install ParetoTI package, this should also install reticulate package, if not - install manually.
install.packages("BiocManager") # for installing BioConductor dependencies
BiocManager::install("vitkl/ParetoTI", dependencies = T)

# Load package
library(ParetoTI)

# Install python dependency into conda python environment and install py_pcha module
ParetoTI::install_py_pcha(method = "conda")
# If no conda manager installed on your machine, try this:
# ParetoTI::install_py_pcha(method = "virtualenv")
# If this fails, install python 2.7 Anaconda distribution, then use 'method = "conda"'.
# Finally, check that is successfully installed and discoverable
reticulate::py_discover_config("py_pcha")
```

## 3. Fitting a polytope to random data and plotting results

This chapter show how to fit a triange (N-dimensional polytope) to the data. This process reflects finding a Pareto front which is indentical to the convex hull of the data (see the proof of this theorem in Uri Alon 2012 Science paper [Supplement, page 24](http://science.sciencemag.org/content/sci/suppl/2012/04/25/science.1217405.DC1/Shoval.SM.v2.pdf)).

```{r random_data}
library(ParetoTI)
library(ggplot2)

# Generate random data that fits into the triangle (3D)
set.seed(4355)
archetypes = generate_arc(arc_coord = list(c(5, 0, 4), # X, Y, Z position of vertices/archetypes
                                           c(-10, 15, 0), c(-30, -20, -5)),
                          mean = 0, sd = 1) # add noise, normal distribution with sd = 1
data = generate_data(archetypes$XC, N_examples = 5*1e2, # specify the number of examples
                     # add some noise and scale so that the data is not a perfect triangle
                     jiiter = 0.04, size = 0.9) 

# First, let's look how much the data visually resembles a triange
# Plot static 2D scatterplot using ggplot2
plot_arc(arch_data = NULL, data = data,
         which_dimensions = 1:2) +
  theme_bw()

# Fit a triangle (k=3 polytope) to those data
arc_data = fit_pch(data, noc = as.integer(3), # number of vertices = 3
                   delta = 0) # parameter that affect tolerance to outliers (0 is robust)

# Show results as interactive 3D scatterplot using plotly
plot_arc(arch_data = arc_data, data = data,
         which_dimensions = 1:3)

# Plot static 2D scatterplot using ggplot2
plot_arc(arch_data = arc_data, data = data,
         which_dimensions = 1:2, 
         nudge = c(0, 0.1), # adjust position of vertex label
         # add extra information to color by
         data_lab = c(rep("p1", ncol(data) * 0.7), 
                      rep("p2", ncol(data) * 0.3)),
         colors = c("#FFC003", "#747171", "#D62728")) +
  theme_bw() +
  xlab("PC1") + ylab("PC2")

# Plot data as 2D density rather than scatterplot
plot_arc(arch_data = arc_data, data = data,
    which_dimensions = 1:2, geom = ggplot2::geom_bin2d) +
  theme_bw()
```

## 4. Evaluating the stability of vertex (archetypes) positions using bootstrap/resampling

Bootstrap or fitting the same shape model to multiple samples of the same data is a useful strategy for understanding how well particular polytope fits the shape of the data and how much certaintly we have in the position of this polytope. A totally different question if how likely you are to obtain the observed shape of the data given no relationship between variables.   
Solution to the second problem is shown in the last chapter of this document. This is done by comparing the t-ratio (polytope / convex hull of the data) of observed to randomised data. Complementary to this, variability in positions of vertices in observed data is compared to random data (measured using bootstrap).  

In this section, we will first evaluate how well PCHA algorhithm converges to an answer. You can reduce conv_crit parameter to 1e-4 or 1e-3 for reduced computation time by more approximate results. 1e-4 gives very similar results to 1e-5 or 1e-6 on simulated example and STARmap 1020 gene data.   
If data has a lot of uncertaintly as of which vertex positions are best match PCHA algorhitm would give variable results.

Note. Algorithm can find the same vertices in a different order so vertexes need to be sorted and arranged according to a common coorditate system. Vertices are ordered by angle (cosine) between c(1, 1) or c(1, 1, ..., 1) vector and a vector pointing to that vertex. When comparing datasets the first dimension may change and thus give a different ordering of archetypes when using 2 dimensions. When bootstraping, shapes are aligned by exploring full space of all possible vertex pairings and choosing the pairing that minimises total distance (align_arc() function).  

```{r convergence}
# Fit polytope 3 times without subsampling to test convergence of the algorithm.
arc_boot_conv = fit_pch_bootstrap(data, n = 3, sample_prop = NULL,
                              noc=as.integer(3), delta=0)
# Show results as interactive 3D scatterplot using plotly
plot_arc(arch_data = arc_boot_conv, data = data,
         which_dimensions = 1:3, type = "all", arch_size = 2)
```

As you can see in this synthetic data, the algorhitm converges to almost identical answers.  

Next, you can evaluate how robust the fit is to variation in the data by subsampling 65% of the data. 

```{r subsampling}
arc_data_boot = fit_pch_bootstrap(data, n = 50, # number of sampling iterations
                              sample_prop = 0.65, # fraction of examples used in each iteration
                              seed = 2543, # seed for random number generation
                              noc=as.integer(3), delta=0, order_type = "align")
plot_arc(arch_data = arc_data_boot, data = data,
         which_dimensions = 1:3, line_size = 1.5)
plot_arc(arch_data = arc_data_boot, data = data,
         which_dimensions = 1:2, line_size = 1) +
  theme_bw()
# Find average positions
arc_data_boot_aver = average_pch_fits(arc_data_boot)
```

You can speed up this procedure using parallelisation across nodes on a local machine (type = "m") or across jobs on a computing cluster (type = "cmq"). Code is shown here but not run.   

```{r subsampling_parallel, eval=FALSE}
# Use local parallel processing to fit the 50 polytopes to resampled datasets each time looking at 65% of examples.
arc_data_boot_m = fit_pch_bootstrap(data, n = 50, sample_prop = 0.65, seed = 2543,
                                order_type = "align", # ordering vertexes by cosine similarity
                                noc=as.integer(3), delta=0, type = "m")
# Use parallel processing on a computing cluster with clustermq to fit the 1000 polytopes
arc_data_boot_cmq = fit_pch_bootstrap(data, n = 1000, sample_prop = 0.65, seed = 2543,
                                  order_type = "align", noc = as.integer(3),
                                  delta = 0, type = "cmq",
                                  # 10 cluster jobs are started with 1000 MB of RAM
                                  clust_options = list(memory = 1000, n_jobs = 10))
# I had a problem using PCHA python package when requesting low memory (500 MB)
```

## 5. Pareto fronts with different number of vertices

Although the example used in this tutorial is an obvious triangle it is commonly useful to evaluate which polytope better fit the data. This section shows how to fit polytopes with 1-4 vertices and evaluate variance explained by these models and sum of squared errors.   

You can clearly see that 2 vertices, a line, are not enough to describe the shape of this data while 4 vertices are not needed. A more formal way of evaluating this is by looking at variance explained by each of those models (shown in the next subsection).    
A complementary approach to find the appropriate the number of vertexes is to evaluate which fit is more stable using fit_pch_bootstrap(). This is due to unstable position of excess points.  

**Fit to shape**

```{r k1_4}
# add - fit to log(data)
arc_ks = k_fit_pch(data, ks = 2:4, check_installed = T,
                   bootstrap = T, bootstrap_N = 20, 
                   bootstrap_type = "m", seed = 2543, volume_ratio = "t_ratio",
                   delta=0, conv_crit = 1e-4, order_type = "align",
                   sample_prop = 0.65, simplex = F)
plot_arc(arch_data = arc_ks, data = data,
         which_dimensions = 1:3, type = "all", arch_size = 2,
         colors = c("grey", "#D62728", "#1F77B4", "#2CA02C", "#17BED0", "#999976"))
plot_arc(arch_data = arc_ks, data = data,
         which_dimensions = 1:2, type = "all", arch_size = 2,
         colors = c("grey", "#D62728", "#1F77B4", "#2CA02C", "#17BED0", "#999976")) +
  theme_bw()
```

**Variance explained for different k**

```{r k1_4_var}
# Show variance explained by a polytope with each k
plot_arc_var(arc_ks, type = "varexpl", point_size = 2, line_size = 1.5) + theme_bw()
# Show variance explained by k-vertex model on top of k-1 model
plot_arc_var(arc_ks, type = "res_varexpl", point_size = 2, line_size = 1.5) + theme_bw()
# Show t-ratio of polytope volume to convex hull volume
plot_arc_var(arc_ks, type = "t_ratio", point_size = 2, line_size = 1.5) + theme_bw()

```

**Stability of the fit**

Excessive vertices will have less stable positions. You can visually see that the position of 4th point varies more than positions of triangle vertices.

```{r stability_of_k, eval=TRUE}
# Show variance in position of vertices obtained using bootstraping
plot_arc_var(arc_ks, type = "total_var", point_size = 2, line_size = 1.5) + theme_bw()

# Fit all vertices again to show variance in position of vertices while varying k
arc_data2 = fit_pch_bootstrap(data, n = 50, sample_prop = 0.65, seed = 2543,
                           order_type = "align", noc = as.integer(2),
                           delta = 0, volume_ratio = "t_ratio", conv_crit = 1e-4, type = "m")
arc_data3 = fit_pch_bootstrap(data, n = 50, sample_prop = 0.65, seed = 2543,
                           order_type = "align", noc = as.integer(3),
                           delta = 0, volume_ratio = "t_ratio", conv_crit = 1e-4, type = "m")
arc_data4 = fit_pch_bootstrap(data, n = 50, sample_prop = 0.65, seed = 2543,
                           order_type = "align", noc = as.integer(4),
                           delta = 0, volume_ratio = "t_ratio", conv_crit = 1e-4, type = "m")
## put this into a function
library(cowplot)
p2 = plot_arc(arc_data2, data, which_dimensions = 1:2, line_size = 1)
p_all = plot_grid(plotlist = list(p2 + theme(legend.position = "none"),
                          plot_arc(arc_data3, data, which_dimensions = 1:2, line_size = 1)+
                            theme(legend.position = "none"),
                          plot_arc(arc_data4, data, which_dimensions = 1:2, line_size = 1)+
                            theme(legend.position = "none"))) 
plot_grid(p_all)
```

## 6. Vertex positions vary a lot when the shape of the data is randomised

```{r randomised, eval=TRUE}
rand3_repl = randomise_fit_pch1(i = 1, data, ks = 3, replace = TRUE,
                                bootstrap_N = 50, seed = 2543,
                           return_data = T, return_arc = T, sample_prop = 0.65,
                           order_type = "align",
                           delta = 0, volume_ratio = "t_ratio",
                           conv_crit = 1e-4, bootstrap_type = "m")

p3 = plot_arc(arc_data3, data, which_dimensions = 1:2, line_size = 1)
p_all = plot_grid(plotlist = list(p3 + theme(legend.position = "none"),
                          plot_arc(rand3_repl$arc_data, rand3_repl$data,
                                   which_dimensions = 1:2, line_size = 1) +
                            theme(legend.position = "none"))) 
plot_grid(p_all)
```

## 7. Randomize variables to measure goodness of observed fit, to calculate empirical p-values for t-ratio and variability in positions

```{r, fig.height=5, fig.width=8, eval=FALSE}
pch_rand = randomise_fit_pch(data, arc_data = arc_ks, n_rand = 1000,
                             replace = FALSE,
                             bootstrap_N = 50,
                             seed = 435,
                             sample_prop = 0.65,
                             volume_ratio = "t_ratio",
                             maxiter = 1000, delta = 0, conv_crit = 1e-4,
                             order_type = "align", type = "cmq",
                             clust_options = list(memory = 2000, n_jobs = 10))
# plot random distributions, observed value (vertical bar) and empirical p-value
plot(pch_rand, nudge_y = 0.5)
# plot background distribution of variance in vertex positions in each dimension and corresponding empirical p-values
plot_dim_var(pch_rand,
             dim_names = c("V1", "V2", "V3"),
             nudge_y = 0.5, nudge_x = 0.5)
```

** Faster version of randomisation that measures t-ratio alone

```{r, fig.height=5, fig.width=8}
pch_rand_t = randomise_fit_pch(data, arc_data = arc_ks, n_rand = 1000,
                             replace = FALSE,
                             bootstrap_N = NA,
                             seed = 435,
                             volume_ratio = "t_ratio",
                             maxiter = 1000, delta = 0, conv_crit = 1e-4,
                             order_type = "align", type = "m") 
# plot random distributions, observed value (vertical bar) and empirical p-value
plot(pch_rand_t, type = "t_ratio", nudge_y = 0.5)
```


## 8. Use Principal Component Analysis to find effective dimensionality of the data, then fit polytope to effective dimensions

This step is necessary for computing the volume of convex hull and t-ratio for a number of reasons:
- Triangle does not have a volume in 3D, so qhull algorhirm will fail to find convel hull of flat 2D shapes in 3D, 3D shapes in 4D and so on. The number of vertices should be number of dimensions plus one.
- Computation time and memory use increse very quickly with dimensions. So, finding a convex hull is not feasible for all genes in a dataset hence the need for principal component space.

```{r dimensionality}
s = svd(data)
arc_data3 = fit_pch_bootstrap(t(s$v[,1:2]), n = 50, sample_prop = 0.65,
                              seed = 2543, order_type = "align", noc = as.integer(3), 
                              delta = 0, type = "m", volume_ratio = "t_ratio")
plot_arc(arc_data3, t(s$v[,1:2]), which_dimensions = 1:2, line_size = 1)
```

## 9. Finding features (gene) whose expression is a decreasing function of distance from vertices

```{r gam}
# average positions obtained using bootstraping in the previous step
arc_pca_aver = average_pch_fits(arc_data3)
# generate fake gene expression data
dist = arch_dist(t(s$v[,1:2]), arc_pca_aver$XC)
data_attr = data.table(gene1 = - 15 * (dist[, 1]) + # enriched near vertex 1
                                   rnorm(nrow(dist), 0, 2) + 15, 
                       gene2 = -(dist[, 2] + # more complex shape
                                   rnorm(nrow(dist), 0, 3) + 5) ^ (1/2) + 
                         (dist[, 3] +
                            rnorm(nrow(dist), 0, 3)) ^ (2))

# find distances of data points to archetypes and merge fake gene expression data
data_attr = merge_arch_dist(arc_pca_aver, t(s$v[,1:2]), t(as.matrix(data_attr)))

# number of points in finite differentiation
n_points = 200
deriv = find_decreasing(data_attr$data, data_attr$arc_col,
                     feature = c("gene1", "gene2"),
                     min.sp = rep(60, length(data_attr$arc_col)), 
                     N_smooths = 4, # cubic spline means 4 parameters
                     n_points = n_points, 
                     d = 1 / n_points, # distance between steps in finite differentiation
                     weights = rep(1, each = n_points), # weight observations differently with distance for calculating p-value 
                     one_arc_per_model = T, # put distances from all vertices as multiple predictors in GAM model (FALSE) or fit separately (TRUE)
                     return_only_summary = F, # Useful when performing more than 10 tests
                     stop_at_10 = FALSE)

# Show GAM model fits 
## x-axis: distance from archetypes, ranks rather than euclidian distance
## y-axis: gene expression
plot_gam(deriv, data = data_attr$data)
# Show corresponding derivatives (y-axis):
plot(deriv) +
  theme(strip.text.y = element_text(angle = 0))

# Show results (p-value)
deriv$summary
```

## 10. Download GO annotations, infer activities of each term in cells using AUCell and find functions decreasing as a function of distance from archetype.

measure_activity() is a convenient wrapper for mapping GO annotations, including propagated to parent terms from children terms, and infering activities of these functions in each cell.   

For more flexibility use map_go_annot(), filter_go_annot(), find_set_activity_AUCell() and find_set_activity_pseudoinv().    

```{r, eval=FALSE}
# Map GO annotations and measure activities
activ = measure_activity(expr_mat, # row names are assumed to be gene identifiers
                         which = "BP",
                         taxonomy_id = 9606, keytype = "ALIAS",
                         ontology_file = load_go_ontology("./data/",
                                                          "goslim_generic.obo"))
# Visually examine activities:
gplots::heatmap.2(as.matrix(activ, rownames = "cells"), trace = "none")

arc_pca_aver = average_pch_fits(arc_data3)
data_attr = merge_arch_dist(arc_pca_aver, t(s$v[,1:2]), expr_mat,
                            colData = activ, colData_id = "cells")

# Find term names for evaluating enrichment
terms = colnames(activ)
# number of points in finite differentiation
n_points = 200
deriv = find_decreasing(data_attr$data, data_attr$arc_col,
                     feature = terms, n_points = 200, 
                     return_only_summary = T) # Useful when performing more than 10 tests
```

## Date and packages used

```{r}
Sys.Date. = Sys.Date()
Sys.Date.
session_info. = devtools::session_info()
session_info.
```
